{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2c3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0bf61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: ../data\n",
      "Files in folder: ['background_recording.pkl', 'action_left_recording.pkl', 'action_down_recording.pkl']\n",
      "Labels found: ['background_recording', 'action_left_recording', 'action_down_recording']\n",
      "Number of labels: 3\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"../data\"\n",
    "print(\"Using data folder:\", data_folder)\n",
    "print(\"Files in folder:\", os.listdir(data_folder))\n",
    "labels = [i.split(\".\")[0] for i in os.listdir(data_folder) if i.endswith(\".pkl\")]\n",
    "print(\"Labels found:\", labels)\n",
    "print(\"Number of labels:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904b0034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in background: 47\n",
      "Number of samples in action_left: 39\n",
      "Number of samples in action_down: 51\n"
     ]
    }
   ],
   "source": [
    "def open_pickle(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "background = open_pickle(os.path.join(data_folder, \"background_recording.pkl\"))\n",
    "action_left = open_pickle(os.path.join(data_folder, \"action_left_recording.pkl\"))\n",
    "action_down = open_pickle(os.path.join(data_folder, \"action_down_recording.pkl\"))\n",
    "\n",
    "print(f\"Number of samples in background: {len(background)}\")\n",
    "print(f\"Number of samples in action_left: {len(action_left)}\")\n",
    "print(f\"Number of samples in action_down: {len(action_down)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e102220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of background: (47, 100, 8)\n",
      "Shape of action_left: (39, 100, 8)\n",
      "Shape of action_down: (51, 100, 8)\n"
     ]
    }
   ],
   "source": [
    "# Crop each sample to have size [100, 8]\n",
    "def crop_sample(data, size=(100, 8)):\n",
    "    new_data = []\n",
    "    for sample in data:\n",
    "        if sample.shape[0] < size[0]:\n",
    "            new_sample = np.zeros((size[0], size[1]))\n",
    "            new_sample[: sample.shape[0], : sample.shape[1]] = sample\n",
    "            new_data.append(new_sample)\n",
    "        else:\n",
    "            new_data.append(sample[: size[0], : size[1]])\n",
    "    return np.array(new_data)\n",
    "\n",
    "\n",
    "background = crop_sample(background)\n",
    "action_left = crop_sample(action_left)\n",
    "action_down = crop_sample(action_down)\n",
    "\n",
    "print(f\"Shape of background: {background.shape}\")\n",
    "print(f\"Shape of action_left: {action_left.shape}\")\n",
    "print(f\"Shape of action_down: {action_down.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc66513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (137, 100, 8)\n",
      "Shape of labels: (137, 1)\n",
      "Shape of data after removing first two columns: (137, 100, 6)\n",
      "Shape of data after flattening: (137, 600)\n",
      "Shape of data after standardization: (137, 600)\n",
      "Shape of X_train: (109, 600)\n",
      "Shape of X_test: (28, 600)\n",
      "Shape of y_train: (109, 1)\n",
      "Shape of y_test: (28, 1)\n"
     ]
    }
   ],
   "source": [
    "background_labels = np.zeros((background.shape[0], 1))\n",
    "action_left_labels = np.ones((action_left.shape[0], 1))\n",
    "action_down_labels = np.full((action_down.shape[0], 1), 2)\n",
    "\n",
    "# Combine all data and labels\n",
    "data = np.concatenate((background, action_left, action_down), axis=0)\n",
    "labels = np.concatenate(\n",
    "    (background_labels, action_left_labels, action_down_labels), axis=0\n",
    ")\n",
    "print(f\"Shape of data: {data.shape}\")\n",
    "print(f\"Shape of labels: {labels.shape}\")\n",
    "\n",
    "# Remove first two columns from each sample (axis=2)\n",
    "data = data[:, :, 2:]\n",
    "print(f\"Shape of data after removing first two columns: {data.shape}\")\n",
    "\n",
    "# Flatten the data so that each sample is a 1D array\n",
    "data = data.reshape(data.shape[0], -1)\n",
    "print(f\"Shape of data after flattening: {data.shape}\")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "print(f\"Shape of data after standardization: {data.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3058d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But data into tensor dataset from torch data and then dataloaders\n",
    "trainset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32),\n",
    ")\n",
    "testset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.float32),\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2889a0",
   "metadata": {},
   "source": [
    "# Simple Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e25d0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=600, out_features=10, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create the model\n",
    "input_size = X_train.shape[1]\n",
    "model = SimpleNN(input_size)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "epochs = 10\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a77ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input size: torch.Size([2, 600])\n",
      "Train label size: torch.Size([2, 1])\n",
      "Test input size: torch.Size([2, 600])\n",
      "Test label size: torch.Size([2, 1])\n",
      "Epoch 1/10, Loss: 2.354554326348594, Accuracy: 81.65%\n",
      "Epoch 2/10, Loss: 1.4692316018502674, Accuracy: 96.33%\n",
      "Epoch 3/10, Loss: 0.39677366042372036, Accuracy: 95.41%\n",
      "Epoch 4/10, Loss: 0.18581341956874398, Accuracy: 96.33%\n",
      "Epoch 5/10, Loss: 0.08371303511971306, Accuracy: 97.25%\n",
      "Epoch 6/10, Loss: 0.08601478717543837, Accuracy: 97.25%\n",
      "Epoch 7/10, Loss: 0.08387824402498956, Accuracy: 97.25%\n",
      "Epoch 8/10, Loss: 0.0828456576168801, Accuracy: 97.25%\n",
      "Epoch 9/10, Loss: 0.08340369000821216, Accuracy: 97.25%\n",
      "Epoch 10/10, Loss: 0.08398327000147442, Accuracy: 97.25%\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# train loop and test loop, calculate accuracy and loss\n",
    "def test_input_sizes(trainloader, testloader):\n",
    "    for inputs, labels in trainloader:\n",
    "        print(f\"Train input size: {inputs.size()}\")\n",
    "        print(f\"Train label size: {labels.size()}\")\n",
    "        break\n",
    "    for inputs, labels in testloader:\n",
    "        print(f\"Test input size: {inputs.size()}\")\n",
    "        print(f\"Test label size: {labels.size()}\")\n",
    "        break\n",
    "\n",
    "\n",
    "def one_hot_target(target_class, predicted_shape):\n",
    "    # Ensure target_class is a torch tensor and flatten it\n",
    "    target_class = target_class.flatten().long()\n",
    "    num_classes = predicted_shape[-1]\n",
    "    num_samples = target_class.shape[0]\n",
    "\n",
    "    one_hot = torch.zeros((num_samples, num_classes), device=target_class.device)\n",
    "    one_hot[torch.arange(num_samples), target_class] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def train(model, trainloader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, one_hot_target(labels, outputs.shape))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            # Calculate accuracy for the epoch\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels.flatten()).sum().item()\n",
    "            total += labels.size(0)\n",
    "        epoch_accuracy = correct / total\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(trainloader)}, Accuracy: {epoch_accuracy * 100:.2f}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "def test(model, testloader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, one_hot_target(labels, outputs.shape))\n",
    "            # Calculate accuracy\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels.flatten()).sum().item()\n",
    "\n",
    "            # print(\n",
    "            #     f\"Batch accuracy: {(preds == labels.flatten()).sum().item() / labels.size(0) * 100:.2f}%\"\n",
    "            # )\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# Check input sizes\n",
    "test_input_sizes(trainloader, testloader)\n",
    "# Train the model\n",
    "train(model, trainloader, criterion, optimizer, epochs)\n",
    "# Test the model\n",
    "test(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1539e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/simple_nn_model.pth\")\n",
    "with open(\"../models/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
